{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKoQACtBDf8"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHMAfZlAA5Yk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKZjL1CRoTLy"
      },
      "outputs": [],
      "source": [
        "train_label=pd.read_csv('/content/new_train.csv')\n",
        "val_label=pd.read_csv('/content/new_val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIdi6gswA5rC"
      },
      "outputs": [],
      "source": [
        "labels = pd.read_csv('/content/new_labels.csv')\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fMDAHx1BGAi"
      },
      "outputs": [],
      "source": [
        "# input_size=224\n",
        "path='/content/all_img/hackathon/'\n",
        "images=[]\n",
        "label_split=[]\n",
        "for i in range(len(train_label)):\n",
        "  row = train_label.iloc[i]\n",
        "  img = cv2.imread(path+row['filename'])\n",
        "\n",
        "  label = sorted(row['labels'].split())\n",
        "  label_split.append(label)\n",
        "  images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZyEIxo1BHqc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "encoding = MultiLabelBinarizer()\n",
        "lab = label_split + [list([str(x) for x in list(labels['id'])])]\n",
        "label_onehot = encoding.fit_transform(lab)\n",
        "label_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6GJ6tdKBIt3"
      },
      "outputs": [],
      "source": [
        "a=label_onehot.tolist()\n",
        "del a[-1]\n",
        "label_onehot = np.array(a)\n",
        "label_onehot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ra_FZNgBJqj"
      },
      "outputs": [],
      "source": [
        "# classes = list(encoding.classes_)\n",
        "#classes from multilabelbinarizer\n",
        "classes = ['0','1','10','100','101','102','103','104','105','106','107','108','109','11','110','111','112','113','114','115','116','117','118','119','12','120','121','122','123','124','125','126','127','128','129','13','130','131','132','133','134','135','136','137','138','139','14','140','141','142','143','144', '145','146','147', '148','149','15','150','151','152','153','154', '155','156','157','158','159','16', '160','161','162','163','164','165','166','167','168','169','17','170','171','172','173','174','175','176','177','178','179','18','180','181','182','183','184','185','186','187','188','189','19','190','191','192','193','194','195','196','197','198','199','2','20','200','201','202','203','204','205', '206','207','208','209','21','210','211','212','213', '214','215','216','217','218','219','22','220','221','222','223', '224','225','226','227','228', '229','23','230','231','232','233','234','235','236','237','238','239','24','240','241','242','243','244','245','246','247','248','249','25','250','251','252','253','254','255','256','257','258','259','26','260','261','262','263','264', '265','266','267','268','269','27','270','271','272','273','274','275','276','277','278','279','28','280','281','282','283','284','285','286','287','288','289', '29','290','291','292','293','294','295','296','297','298','299','3','30','300','301','31','32','34','35','36','37','38','39','4','40','41','42','43','44','45','46','47','48','49','5','50','51','52','53','54','55','56','57','58','59','6','60','61','62','63','64','65','66','67','68','69','7','70','71','72','73','74','75','76','77','78','79','8','80','81','82','83','84','85','86','87','88','89','9','90','91','92','93','94','95','96','97', '98','99']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RJNdLqHMUz3"
      },
      "outputs": [],
      "source": [
        "datagen=ImageDataGenerator(rescale=1./255.\n",
        "                            featurewise_center=True,\n",
        "                            featurewise_std_normalization=True,\n",
        "                            rotation_range=20,\n",
        "                            width_shift_range=0.2,\n",
        "                            height_shift_range=0.2,\n",
        "                            horizontal_flip=True)\n",
        "\n",
        "train_generator=datagen.flow_from_dataframe(dataframe=train_label, directory=path, x_col=\"filename\", y_col=\"labels\", \n",
        "                                            subset=\"training\", batch_size=256, seed=42,classes=classes, shuffle=True, \n",
        "                                            class_mode=\"categorical\", target_size=(224,224))\n",
        "\n",
        "valid_generator=datagen.flow_from_dataframe( dataframe=val_label, directory=path, x_col=\"filename\", y_col=\"labels\", \n",
        "                                            subset=\"training\", batch_size=256, seed=42,classes=classes, shuffle=True, \n",
        "                                            class_mode=\"categorical\", target_size=(224,224))\n",
        "\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(dataframe=df_test, directory=\"./hackathon/\", x_col=\"filename\", \n",
        "                                                y_col=None, batch_size=256, seed=42, \n",
        "                                                shuffle=False, class_mode=None, target_size=(224,224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1buc14GrBnRS"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdUwWLSlBpAa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers\n",
        "IMG_SIZE=224\n",
        "out_dim = 302\n",
        "\n",
        "pretrain = tf.keras.applications.EfficientNetB0(include_top=False,weights=\"imagenet\")\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = pretrain(inputs)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dense(out_dim, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0asonNbBuZh"
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt,metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm0kjAotRbUI"
      },
      "outputs": [],
      "source": [
        "filepath='/content/my_best_model.hdf5'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_loss')\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1,min_delta=0.001)\n",
        "\n",
        "model.compile(optimizer =tf.keras.optimizers.Adam(learning_rate=1e-2), loss = 'binary_crossentropy',metrics = ['acc'])\n",
        "batch_size = 64\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=valid_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,callbacks=[callbacks,checkpoint],\n",
        "                    epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDlC0GvBl5nD"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egp2Zyo6Spdw"
      },
      "outputs": [],
      "source": [
        "##Predict and submission on kaggle ##\n",
        "proba = model.predict(test_generator)\n",
        "submission=[]\n",
        "for i,p in enumerate(proba):\n",
        "  index_p=np.where(proba[i] >0.3)\n",
        "  class_pred=[]\n",
        "  for j in index_p[0]:\n",
        "    k = int(classes[int(j)])\n",
        "    # print(j, labels['label'].iloc[j])\n",
        "    class_pred.append(k)\n",
        "    submission.append(sorted(class_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZM35DhXJA8_y",
        "2mv9hl13BLEE"
      ],
      "name": "Copy of effB7_bypop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
